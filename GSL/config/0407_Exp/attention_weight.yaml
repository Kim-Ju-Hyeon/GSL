---
exp_name: Attention_Weight
exp_dir: ../exp
use_gpu: True
device: 0
seed: 1010
model_name: model_name

nodes_num: 325
node_features: 2
hidden_dim: 32
encoder_embedding_dim: 2
decoder_embedding_dim: 1
encoder_step: 12
decoder_step: 3

dataset:
  root: ./data/PEMS-BAY
  name: PEMS-BAY
  graph_learning_length: 52105
  pred_step: 1

train:
  optimizer: Adam
  epoch: 100
  loss_function : MAE
  lr: 1.0e-3
  momentum: 0.9
  wd: 0.0e-4
  batch_size : 8
  lr_decay: 0.1
  lr_decay_steps: [10000]

graph_learning:
  mode: attention # GTS, attention, MTGNN
  sampling: None # Gumbel_softmax, Top_k, None
  tau: 0.3
  top_k: 10
  alpha: 200
  to_symmetric: False
  initial_edge_index: Fully Connected
  sequence: 1

  kernel_size: [10, 10]
  stride: [1, 1]
  conv_dim: [8, 16]
  n_head: 4
  hidden_dim: 128


forecasting_module:
  diffusion_k: 2
  num_layer: 1
  use_teacher_forcing: True
  teacher_forcing_ratio: 0.5



